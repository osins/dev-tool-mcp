#!/usr/bin/env python3
"""
Spider MCP å®¢æˆ·ç«¯å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼
"""

import asyncio
import os
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# é…ç½®æœåŠ¡å™¨è¿æ¥å‚æ•°ï¼ˆè¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent
SERVER_PARAMS = StdioServerParameters(
    command=str(PROJECT_ROOT / "venv" / "bin" / "python"),
    args=[str(PROJECT_ROOT / "spider_mcp_server" / "server.py")]
)

async def crawl_webpage_simple():
    """ç®€å•çš„ç½‘é¡µçˆ¬å–ç¤ºä¾‹"""
    print("ğŸ•·ï¸ å¼€å§‹çˆ¬å–ç½‘é¡µ...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "./simple_output"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # è¿æ¥åˆ°MCPæœåŠ¡å™¨
        async with stdio_client(SERVER_PARAMS) as (read, write):
            async with ClientSession(read, write) as session:
                # åˆå§‹åŒ–ä¼šè¯
                await session.initialize()
                
                # è°ƒç”¨çˆ¬è™«å·¥å…·
                result = await session.call_tool("crawl_web_page", {
                    "url": "https://github.com/unclecode/crawl4ai",
                    "save_path": output_dir
                })
                
                # âœ… æ­£ç¡®å¤„ç†è¿”å›ç»“æœ
                # MCPæœåŠ¡å™¨è¿”å›çš„æ˜¯ CallToolResult å¯¹è±¡ï¼Œå†…å®¹åœ¨ result.content ä¸­
                for content in result.content:
                    if content.type == "text":
                        print(f"âœ… çˆ¬å–ç»“æœ: {content.text}")
                
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(crawl_webpage_simple())