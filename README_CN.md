# MCP å¼€å‘å·¥å…·æœåŠ¡å™¨

åŸºäº [crawl4ai](https://github.com/unclecode/crawl4ai) å’Œ Playwright çš„ MCP (Model Context Protocol) å¼€å‘å·¥å…·æœåŠ¡å™¨ï¼Œæä¾›å¼ºå¤§çš„ç½‘ç»œçˆ¬å–ã€å†…å®¹æå–å’Œæµè§ˆå™¨è‡ªåŠ¨åŒ–åŠŸèƒ½ã€‚

## ğŸš€ ç‰¹æ€§

- **æ™ºèƒ½ç½‘ç»œçˆ¬è™«**: åŸºäº crawl4ai çš„é«˜æ•ˆç½‘é¡µå†…å®¹æå–
- **LLM å¢å¼ºæå–**: é›†æˆå¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒæ™ºèƒ½å†…å®¹ç†è§£å’Œç»“æ„åŒ–æå–
- **å¤šæ ¼å¼è¾“å‡º**: æ”¯æŒ HTMLã€Markdownã€JSONã€PDF å’Œ PNG æ ¼å¼
- **æ™ºèƒ½å†…å®¹æå–**: ä½¿ç”¨ LLMExtractionStrategy è¿›è¡ŒåŸºäºè¯­ä¹‰çš„å†…å®¹æå–
- **çµæ´»é…ç½®**: æ”¯æŒä¼ ç»Ÿ CSS é€‰æ‹©å™¨æå–å’Œ LLM æ™ºèƒ½æå–ä¸¤ç§æ¨¡å¼
- **æˆªå›¾åŠŸèƒ½**: è‡ªåŠ¨ç”Ÿæˆç½‘é¡µæˆªå›¾
- **PDF å¯¼å‡º**: å°†ç½‘é¡µå†…å®¹å¯¼å‡ºä¸º PDF æ–‡ä»¶
- **å†…å®¹è¿‡æ»¤**: ä½¿ç”¨ PruningContentFilter ä¼˜åŒ–å†…å®¹æå–
- **ç»“æ„åŒ–æ•°æ®æå–**: æ”¯æŒ JsonCssExtractionStrategy ç²¾ç¡®æ•°æ®æå–
- **æ–‡ä»¶ä¸‹è½½**: è‡ªåŠ¨ä¸‹è½½å¹¶ä¿å­˜å¼•ç”¨æ–‡ä»¶
- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ Ollamaã€OpenAIã€Claude ç­‰å¤šç§ LLM æä¾›å•†
- **ç¯å¢ƒå˜é‡é…ç½®**: çµæ´»çš„æ¨¡å‹é…ç½®å’Œåˆ‡æ¢æœºåˆ¶
- **MCP åè®®æ”¯æŒ**: å®Œå…¨å…¼å®¹ MCP æ ‡å‡†ï¼Œå¯é›†æˆåˆ°æ”¯æŒ MCP çš„å®¢æˆ·ç«¯
- **æµè§ˆå™¨è‡ªåŠ¨åŒ–**: Playwright é©±åŠ¨çš„é«˜çº§æµè§ˆå™¨åŠŸèƒ½
- **é¡µé¢å†…å®¹åˆ†æ**: è·å–å®Œæ•´çš„ HTMLã€æ–‡æœ¬ã€å…ƒæ•°æ®ã€é“¾æ¥å’Œå›¾ç‰‡
- **æ§åˆ¶å°æ¶ˆæ¯æ•è·**: ç›‘æ§ JavaScript æ—¥å¿—ã€è­¦å‘Šå’Œé”™è¯¯
- **ç½‘ç»œè¯·æ±‚è·Ÿè¸ª**: è®°å½•é¡µé¢å‘èµ·çš„æ‰€æœ‰ç½‘ç»œè¯·æ±‚å’Œå“åº”
- **å®æ—¶æµå¼å¤„ç†**: å®æ—¶æµå¼å¤„ç†çŠ¶æ€å’Œä¸­é—´ç»“æœ

## ğŸ“¦ å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- æ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
- **LLM ä¾èµ–é¡¹**:
  - `litellm` - ç»Ÿä¸€çš„å¤šæ¨¡å‹ LLM æ¥å£
  - Ollama æˆ–å…¶ä»– LLM æä¾›å•†ï¼ˆå¯é€‰ï¼Œç”¨äºæ™ºèƒ½å†…å®¹æå–ï¼‰
- **æµè§ˆå™¨ä¾èµ–é¡¹**:
  - Chrome/Chromiumï¼ˆcrawl4ai éœ€è¦ï¼‰
  - Playwrightï¼ˆè‡ªåŠ¨å®‰è£…ï¼‰

### å®‰è£…æ­¥éª¤

1. **å…‹éš†ä»“åº“**
```bash
git clone https://github.com/osins/dev-tool-mcp.git
cd dev-tool-mcp
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows
```

3. **å®‰è£…ä¾èµ–**
```bash
pip install -e .
```

## ğŸ”§ MCP æœåŠ¡é…ç½®

### Claude Desktop é…ç½®ç¤ºä¾‹

å°†ä»¥ä¸‹é…ç½®æ·»åŠ åˆ° Claude Desktop çš„é…ç½®æ–‡ä»¶ä¸­ï¼š

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "dev-tool": {
      "command": "/path/to/dev-tool-mcp/venv/bin/python",
      "args": [
        "/path/to/dev-tool-mcp/mcp_server/server.py"
      ],
      "description": "ä½¿ç”¨ crawl4ai è¿›è¡Œç½‘é¡µçˆ¬å–å’Œå†…å®¹æå–çš„ MCP å¼€å‘å·¥å…·æœåŠ¡å™¨"
    }
  }
}
```

### é€šç”¨ MCP å®¢æˆ·ç«¯é…ç½®

å¦‚æœä½¿ç”¨å…¶ä»– MCP å®¢æˆ·ç«¯ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹é€šç”¨é…ç½®ï¼š

```json
{
  "servers": {
    "dev-tool-crawler": {
      "name": "å¼€å‘å·¥å…·çˆ¬å–æœåŠ¡å™¨",
      "description": "ç½‘é¡µçˆ¬å–å’Œå†…å®¹æå–æœåŠ¡å™¨",
      "command": "/path/to/dev-tool-mcp/venv/bin/python",
      "args": [
        "/path/to/dev-tool-mcp/mcp_server/server.py"
      ],
      "timeout": 30000
    }
  }
}
```

### ğŸ“‹ é…ç½®è¯´æ˜

**ç›´æ¥è„šæœ¬æ‰§è¡Œå³å¯:**
- æ— éœ€ç¯å¢ƒå˜é‡
- æ— éœ€ä½¿ç”¨ `-m` å‚æ•°
- Python è‡ªåŠ¨å¤„ç†ç›¸å¯¹å¯¼å…¥
- æœ€ç®€å•æœ€å¯é çš„é…ç½®

### ğŸ¤– LLM é…ç½®é€‰é¡¹

ä¸ºå¯ç”¨ LLM å¢å¼ºåŠŸèƒ½ï¼Œå¯è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# å¯ç”¨ LLM æ¨¡å¼
export CRAWL_MODE=llm

# LLM æä¾›å•†é…ç½®
export LLAMA_PROVIDER="ollama/qwen2.5-coder:latest"  # é»˜è®¤å€¼
export LLAMA_API_TOKEN="your_api_token"             # å¯é€‰ï¼ŒæŸäº›æä¾›å•†éœ€è¦
export LLAMA_BASE_URL="http://localhost:11434"       # å¯é€‰ï¼Œè‡ªå®šä¹‰ API ç«¯ç‚¹
export LLAMA_MAX_TOKENS=4096                         # å¯é€‰ï¼Œæœ€å¤§ token æ•°
```

**æ”¯æŒçš„ LLM æä¾›å•†:**
- **Ollama**: `ollama/model-name` (æœ¬åœ°éƒ¨ç½²)
- **OpenAI**: `openai/gpt-4` / `openai/gpt-3.5-turbo`
- **Claude**: `anthropic/claude-3-sonnet`
- **å…¶ä»–**: é€šè¿‡ litellm æ”¯æŒçš„æ‰€æœ‰æä¾›å•†

## ğŸ› ï¸ MCP åè®®ä½¿ç”¨æŒ‡å—

### åŸºäº MCP åè®®çš„å®¢æˆ·ç«¯å¼€å‘

æ­¤é¡¹ç›®åŸºäº [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) åè®®ï¼Œæä¾›æ ‡å‡†åŒ–å·¥å…·è°ƒç”¨æ¥å£ã€‚ç¼–å†™ MCP å®¢æˆ·ç«¯çš„å…³é”®è¦ç‚¹å¦‚ä¸‹ï¼š

#### 1. MCP è¿æ¥å»ºç«‹

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# é…ç½®æœåŠ¡å™¨å‚æ•°
server_params = StdioServerParameters(
    command="/path/to/venv/bin/python",  # Python è§£é‡Šå™¨è·¯å¾„
    args=["/path/to/server.py"]         # æœåŠ¡å™¨è„šæœ¬è·¯å¾„
)

# å»ºç«‹ stdio è¿æ¥
async with stdio_client(server_params) as (read, write):
    async with ClientSession(read, write) as session:
        await session.initialize()  # åˆå§‹åŒ–ä¼šè¯
```

#### 2. å·¥å…·è°ƒç”¨å’Œç»“æœå¤„ç†

**âš ï¸ é‡è¦: MCP è¿”å›å€¼ç»“æ„**

MCP æœåŠ¡å™¨è¿”å› `CallToolResult` å¯¹è±¡ï¼Œå®é™…å†…å®¹åœ¨ `result.content` ä¸­ï¼š

```python
# âŒ é”™è¯¯æ–¹æ³•ï¼ˆå¸¸è§é”™è¯¯ï¼‰
for content in result:  # result ä¸å¯è¿­ä»£
    print(content.text)

# âœ… æ­£ç¡®æ–¹æ³•
result = await session.call_tool("tool_name", {"param": "value"})
for content in result.content:  # è®¿é—® content å±æ€§
    if content.type == "text":  # æ£€æŸ¥å†…å®¹ç±»å‹
        print(content.text)
```

#### 3. æ­¤é¡¹ç›®å¯ç”¨çš„å·¥å…·æ¥å£

**å¯ç”¨å·¥å…·:**
- `say_hello` - æµ‹è¯•è¿é€šæ€§
- `echo_message` - å›æ˜¾æ¶ˆæ¯
- `crawl_web_page` - ç½‘é¡µçˆ¬å–
- `get_page_content` - é€šè¿‡ URL è·å–é¡µé¢å†…å®¹
- `get_console_messages` - è·å–é¡µé¢æ§åˆ¶å°æ¶ˆæ¯
- `get_network_requests` - è·å–é¡µé¢å‘èµ·çš„ç½‘ç»œè¯·æ±‚

**crawl_web_page å·¥å…·å‚æ•°:**
```python
{
    "url": "https://example.com",           # è¦çˆ¬å–çš„ URL
    "save_path": "./output_directory"       # ä¿å­˜è·¯å¾„
}
```

**è¿”å›å€¼å¤„ç†:**
```python
result = await session.call_tool("crawl_web_page", {
    "url": "https://github.com/unclecode/crawl4ai",
    "save_path": "./results"
})

# æ­£ç¡®è§£æè¿”å›ç»“æœ
for content in result.content:
    if content.type == "text":
        message = content.text
        print(f"çˆ¬å–ç»“æœ: {message}")

        # æ¶ˆæ¯æ ¼å¼ç¤ºä¾‹:
        # "Successfully crawled https://github.com/unclecode/crawl4ai and saved 8 files to ./results/20231119-143022"
```

#### 4. é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

```python
async def safe_crawl(session: ClientSession, url: str, save_path: str):
    try:
        result = await session.call_tool("crawl_web_page", {
            "url": url,
            "save_path": save_path
        })

        # æ£€æŸ¥è¿”å›ç»“æœ
        if result.content:
            for content in result.content:
                if content.type == "text":
                    if "Failed to crawl" in content.text:
                        print(f"âŒ çˆ¬å–å¤±è´¥: {content.text}")
                    else:
                        print(f"âœ… çˆ¬å–æˆåŠŸ: {content.text}")
        else:
            print("âŒ æœªæ”¶åˆ°è¿”å›ç»“æœ")

    except Exception as e:
        print(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
```

## ğŸ› ï¸ å¯ç”¨å·¥å…·

### 1. `crawl_web_page`

ä»æŒ‡å®š URL çˆ¬å–ç½‘é¡µå†…å®¹ï¼Œå¹¶ä»¥å¤šç§æ ¼å¼ä¿å­˜ã€‚æ”¯æŒä¼ ç»Ÿ CSS æå–å’Œ LLM å¢å¼ºæå–æ¨¡å¼ã€‚

**å‚æ•°:**
- `url` (string, required): è¦çˆ¬å–çš„ç½‘é¡µ URL
- `save_path` (string, required): ä¿å­˜çˆ¬å–å†…å®¹çš„ç›®å½•è·¯å¾„
- `instruction` (string, optional): ç”¨äº LLM çš„æŒ‡ä»¤ (é»˜è®¤: DEFAULT_INSTRUCTION)
- `save_screenshot` (boolean, optional): ä¿å­˜é¡µé¢æˆªå›¾ (é»˜è®¤: False)
- `save_pdf` (boolean, optional): ä¿å­˜é¡µé¢ PDF (é»˜è®¤: False)
- `generate_markdown` (boolean, optional): ç”Ÿæˆé¡µé¢ Markdown è¡¨ç¤º (é»˜è®¤: False)

**åŠŸèƒ½:**
- è‡ªåŠ¨ç½‘é¡µæˆªå›¾ (PNG)
- PDF å¯¼å‡ºç”Ÿæˆ
- åŸå§‹ Markdown å†…å®¹æå–
- æ¸…æ´/è¿‡æ»¤çš„ Markdown å†…å®¹
- ç»“æ„åŒ–æ•°æ®æå– (JSON)
- HTML å†…å®¹ä¿å­˜
- ä¸‹è½½æ–‡ä»¶å¤„ç†
- **LLM æ™ºèƒ½æå–** (å½“è®¾ç½® CRAWL_MODE=llm æ—¶å¯ç”¨):
  - åŸºäºè¯­ä¹‰çš„å†…å®¹ç†è§£
  - è‡ªåŠ¨å»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰éä¸»è¦å†…å®¹
  - ç»“æ„åŒ–çš„ Markdown è¾“å‡º
  - æ”¯æŒå¤šç§ LLM æä¾›å•†

**ç¤ºä¾‹ç”¨æ³•:**
```python
# ä¼ ç»Ÿçˆ¬å–æ¨¡å¼
result = await session.call_tool("crawl_web_page", {
    "url": "https://example.com",
    "save_path": "./output_directory"
})

# LLM å¢å¼ºæ¨¡å¼ (è®¾ç½®ç¯å¢ƒå˜é‡)
os.environ["CRAWL_MODE"] = "llm"
os.environ["LLAMA_PROVIDER"] = "ollama/qwen2.5-coder:latest"
os.environ["LLAMA_BASE_URL"] = "http://localhost:11434"
```

### 2. `get_page_content`

é€šè¿‡ URL è·å–ç½‘é¡µå†…å®¹å¹¶å®æ—¶åˆ†æã€‚

**å‚æ•°:**
- `url` (string, required): è¦è·å–å†…å®¹çš„ç½‘é¡µ URL
- `wait_for_selector` (string, optional): è·å–å†…å®¹å‰ç­‰å¾…ç‰¹å®šå…ƒç´ çš„ CSS é€‰æ‹©å™¨
- `wait_timeout` (integer, optional): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 30000

**è¿”å›:**
åŒ…å«ä»¥ä¸‹å­—æ®µçš„ JSON å¯¹è±¡ï¼š
- `url`: è¯·æ±‚çš„ URL
- `status`: HTTP çŠ¶æ€ç 
- `title`: é¡µé¢æ ‡é¢˜
- `html`: é¡µé¢ HTML å†…å®¹
- `text`: é¡µé¢æ–‡æœ¬å†…å®¹
- `meta`: é¡µé¢å…ƒæ•°æ®
- `links`: é¡µé¢é“¾æ¥åˆ—è¡¨
- `images`: é¡µé¢å›¾ç‰‡åˆ—è¡¨
- `timestamp`: æ“ä½œæ—¶é—´æˆ³

### 3. `get_console_messages`

é€šè¿‡ URL ä»ç½‘é¡µè·å–æ§åˆ¶å°æ¶ˆæ¯ (æ—¥å¿—ã€è­¦å‘Šã€é”™è¯¯)ã€‚

**å‚æ•°:**
- `url` (string, required): è¦è·å–æ§åˆ¶å°æ¶ˆæ¯çš„ç½‘é¡µ URL
- `wait_for_selector` (string, optional): è·å–æ§åˆ¶å°æ¶ˆæ¯å‰ç­‰å¾…ç‰¹å®šå…ƒç´ çš„ CSS é€‰æ‹©å™¨
- `wait_timeout` (integer, optional): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 30000

**è¿”å›:**
åŒ…å«ä»¥ä¸‹å­—æ®µçš„ JSON å¯¹è±¡ï¼š
- `url`: è¯·æ±‚çš„ URL
- `status`: HTTP çŠ¶æ€ç 
- `console_messages`: æ§åˆ¶å°æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯æ¡æ¶ˆæ¯åŒ…å«ç±»å‹ã€æ–‡æœ¬ã€ä½ç½®å’Œå †æ ˆä¿¡æ¯
- `timestamp`: æ“ä½œæ—¶é—´æˆ³

### 4. `get_network_requests`

é€šè¿‡ URL è·å–é¡µé¢å‘èµ·çš„ç½‘ç»œè¯·æ±‚ã€‚

**å‚æ•°:**
- `url` (string, required): è¦è·å–ç½‘ç»œè¯·æ±‚çš„ç½‘é¡µ URL
- `wait_for_selector` (string, optional): è·å–ç½‘ç»œè¯·æ±‚å‰ç­‰å¾…ç‰¹å®šå…ƒç´ çš„ CSS é€‰æ‹©å™¨
- `wait_timeout` (integer, optional): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 30000

**è¿”å›:**
åŒ…å«ä»¥ä¸‹å­—æ®µçš„ JSON å¯¹è±¡ï¼š
- `url`: è¯·æ±‚çš„ URL
- `status`: HTTP çŠ¶æ€ç 
- `requests`: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚åŒ…å« URLã€æ–¹æ³•ã€èµ„æºç±»å‹ç­‰ä¿¡æ¯
- `responses`: å“åº”åˆ—è¡¨ï¼Œæ¯ä¸ªå“åº”åŒ…å« URLã€çŠ¶æ€ç ã€å¤´ä¿¡æ¯ç­‰
- `total_requests`: æ€»è¯·æ±‚æ•°
- `total_responses`: æ€»å“åº”æ•°
- `timestamp`: æ“ä½œæ—¶é—´æˆ³

### 5. `say_hello`

ç”¨äºæµ‹è¯•æœåŠ¡å™¨è¿é€šæ€§çš„ç®€å•é—®å€™å·¥å…·ã€‚

**å‚æ•°:**
- `name` (string, optional): è¦é—®å€™çš„åå­—ï¼Œé»˜è®¤ä¸º "World"

**ç¤ºä¾‹ç”¨æ³•:**
```python
result = await session.call_tool("say_hello", {
    "name": "Alice"
})
```

### 6. `echo_message`

ç”¨äºæµ‹è¯•é€šä¿¡çš„å›æ˜¾æ¶ˆæ¯å·¥å…·ã€‚

**å‚æ•°:**
- `message` (string, required): è¦å›æ˜¾çš„æ¶ˆæ¯

**ç¤ºä¾‹ç”¨æ³•:**
```python
result = await session.call_tool("echo_message", {
    "message": "Hello MCP!"
})
```

## ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„

çˆ¬å–å®Œæˆåï¼Œå°†åœ¨æŒ‡å®šçš„è¾“å‡ºç›®å½•ä¸­ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
output_directory/
â”œâ”€â”€ output.html              # å®Œæ•´ HTML å†…å®¹
â”œâ”€â”€ output.json              # ç»“æ„åŒ–æ•°æ® (CSS æå–çš„ JSON)
â”œâ”€â”€ output.png               # ç½‘é¡µæˆªå›¾
â”œâ”€â”€ output.pdf               # ç½‘é¡µçš„ PDF ç‰ˆæœ¬
â”œâ”€â”€ raw_markdown.md          # åŸå§‹ markdown æå–
â”œâ”€â”€ fit_markdown.md          # æ¸…æ´/è¿‡æ»¤çš„ markdown
â”œâ”€â”€ downloaded_files.json    # ä¸‹è½½æ–‡ä»¶åˆ—è¡¨ (å¦‚æœ‰)
â””â”€â”€ files/                   # ä¸‹è½½æ–‡ä»¶ç›®å½• (å¦‚æœ‰)
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ç½‘é¡µçˆ¬å–

```python
import asyncio
import os
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def crawl_example():
    # é…ç½®æœåŠ¡å™¨è¿æ¥å‚æ•° (æ ¹æ®éœ€è¦è°ƒæ•´è·¯å¾„)
    project_root = Path("/path/to/your/dev-tool-mcp")
    server_params = StdioServerParameters(
        command=str(project_root / "venv" / "bin" / "python"),
        args=[str(project_root / "mcp_server" / "server.py")]
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "./crawl_results"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # è¿æ¥ MCP æœåŠ¡å™¨
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # åˆå§‹åŒ–ä¼šè¯
                await session.initialize()

                # è°ƒç”¨çˆ¬å–å·¥å…·
                result = await session.call_tool("crawl_web_page", {
                    "url": "https://github.com/unclecode/crawl4ai",
                    "save_path": output_dir
                })

                # âœ… æ­£ç¡®çš„è¿”å›å€¼å¤„ç†
                # MCP æœåŠ¡å™¨è¿”å› CallToolResult å¯¹è±¡ï¼Œå†…å®¹åœ¨ result.content ä¸­
                for content in result.content:
                    if content.type == "text":
                        print(f"âœ… çˆ¬å–ç»“æœ: {content.text}")

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")

# è¿è¡Œç¤ºä¾‹
asyncio.run(crawl_example())
```

### æµè§ˆå™¨å†…å®¹åˆ†æ

```python
# è·å–å®Œæ•´é¡µé¢å†…å®¹
result = await session.call_tool("get_page_content", {
    "url": "https://nextjs.org",
    "wait_for_selector": "main",
    "wait_timeout": 15000
})

# è·å–æ§åˆ¶å°æ¶ˆæ¯ (æ£€æµ‹ JavaScript é”™è¯¯å¾ˆæœ‰ç”¨)
console_result = await session.call_tool("get_console_messages", {
    "url": "https://example.com",
    "wait_for_selector": ".app-loaded",
    "wait_timeout": 10000
})

# è·å–ç½‘ç»œè¯·æ±‚ (API è·Ÿè¸ªå¾ˆæœ‰ç”¨)
network_result = await session.call_tool("get_network_requests", {
    "url": "https://api.example.com",
    "wait_for_selector": "[data-loaded=true]",
    "wait_timeout": 20000
})
```

### æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘é¡µ

```python
urls = [
    "https://example.com",
    "https://github.com",
    "https://stackoverflow.com"
]

for i, url in enumerate(urls):
    result = await session.call_tool("crawl_web_page", {
        "url": url,
        "save_path": f"./results/crawl_{i+1}"
    })

    # âœ… æ­£ç¡®çš„è¿”å›å€¼å¤„ç†
    for content in result.content:
        if content.type == "text":
            print(f"çˆ¬å–å®Œæˆ: {url} - {content.text}")

    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
    await asyncio.sleep(2)
```

### LLM å¢å¼ºçˆ¬å–ç¤ºä¾‹

```python
import os
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def llm_enhanced_crawl():
    # é…ç½® LLM ç¯å¢ƒå˜é‡
    os.environ["CRAWL_MODE"] = "llm"
    os.environ["LLAMA_PROVIDER"] = "ollama/qwen2.5-coder:latest"
    os.environ["LLAMA_BASE_URL"] = "http://localhost:11434"

    # é…ç½®æœåŠ¡å™¨è¿æ¥
    server_params = StdioServerParameters(
        command="/path/to/venv/bin/python",
        args=["/path/to/mcp_server/server.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # LLM å¢å¼ºçˆ¬å–
            result = await session.call_tool("crawl_web_page", {
                "url": "https://example.com/article",
                "save_path": "./llm_results"
            })

            for content in result.content:
                if content.type == "text":
                    print(f"LLM å¢å¼ºçˆ¬å–ç»“æœ: {content.text}")

asyncio.run(llm_enhanced_crawl())
```

## ğŸ§ª å¼€å‘å’Œæµ‹è¯•

### è¿è¡Œæµ‹è¯•

é¡¹ç›®åŒ…å«å…¨é¢çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
# è¿è¡Œå®Œæ•´çˆ¬è™«æµ‹è¯•ï¼Œå¸¦çœŸå®æ–‡ä»¶è¾“å‡º
python test/test_complete_crawler.py

# è¿è¡Œå•ä¸ªç»„ä»¶æµ‹è¯•
python test/test_hello.py      # æµ‹è¯• hello/echo åŠŸèƒ½
python test/test_server.py     # æµ‹è¯• MCP æœåŠ¡å™¨åŠŸèƒ½
python test/test_crawl.py      # æµ‹è¯•çˆ¬å–åŠŸèƒ½
python test/test_complete.py    # æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹
python test/test_browser.py     # æµ‹è¯•æµè§ˆå™¨è‡ªåŠ¨åŒ–åŠŸèƒ½
```

### æµ‹è¯•ç›®å½•ç»“æ„

```
test/
â”œâ”€â”€ test_complete_crawler.py    # å®Œæ•´é›†æˆæµ‹è¯•
â”œâ”€â”€ test_hello.py             # Hello/echo åŠŸèƒ½
â”œâ”€â”€ test_server.py            # MCP æœåŠ¡å™¨åè®®
â”œâ”€â”€ test_crawl.py             # æ ¸å¿ƒçˆ¬å–é€»è¾‘
â”œâ”€â”€ test_browser.py           # æµè§ˆå™¨è‡ªåŠ¨åŒ–æµ‹è¯•
â””â”€â”€ test_complete.py          # ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹
```

### å¼€å‘æ¨¡å¼

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# é¡¹ç›®ä½¿ç”¨ pyright è¿›è¡Œç±»å‹æ£€æŸ¥ (åœ¨ pyproject.toml ä¸­é…ç½®)
# æœªé…ç½®å…¶ä»– linting å·¥å…·
```

## ğŸ“š é¡¹ç›®ç»“æ„

```
dev-tool-mcp/
â”œâ”€â”€ mcp_server/          # ä¸»åŒ…
â”‚   â”œâ”€â”€ __init__.py            # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ server.py              # MCP æœåŠ¡å™¨å®ç°
â”‚   â”œâ”€â”€ crawl.py              # çˆ¬å–é€»è¾‘å’Œæ–‡ä»¶å¤„ç†
â”‚   â”œâ”€â”€ llm.py                # LLM é…ç½®å’Œæå–ç­–ç•¥
â”‚   â”œâ”€â”€ browser/              # æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py       # æµè§ˆå™¨æ¨¡å—åˆå§‹åŒ–
â”‚   â”‚   â”œâ”€â”€ browser_service.py # æµè§ˆå™¨æœåŠ¡å®ç°
â”‚   â”‚   â””â”€â”€ README.md         # æµè§ˆå™¨æ¨¡å—æ–‡æ¡£
â”‚   â””â”€â”€ utils.py              # æ–‡ä»¶ I/O å®ç”¨å‡½æ•°
â”œâ”€â”€ test/                     # æµ‹è¯•å¥—ä»¶
â”‚   â”œâ”€â”€ test_litellm_ollama.py # LLM é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_browser.py       # æµè§ˆå™¨åŠŸèƒ½æµ‹è¯•
â”‚   â””â”€â”€ ...                   # å…¶ä»–æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ test_output/              # æµ‹è¯•è¾“å‡ºç›®å½•
â”œâ”€â”€ typings/                  # crawl4ai çš„ç±»å‹å­˜æ ¹
â”œâ”€â”€ pyproject.toml            # é¡¹ç›®é…ç½®
â””â”€â”€ README.md                # æ­¤æ–‡ä»¶
```

## ğŸ“š API å‚è€ƒ

### æ ¸å¿ƒç±»å’Œå‡½æ•°

#### `llm_config()` å‡½æ•° (`llm.py`)
é…ç½® LLM å¢å¼ºçˆ¬å–ç­–ç•¥ã€‚

**å‚æ•°:**
- `instruction` (str): æå–æŒ‡ä»¤ï¼Œé»˜è®¤ä¸ºä¸“é—¨ä¼˜åŒ–çš„ç½‘é¡µå†…å®¹æå–æŒ‡ä»¤

**è¿”å›:**
- `CrawlerRunConfig`: é…ç½®äº† LLM æå–ç­–ç•¥çš„çˆ¬å–é…ç½®

**ç‰¹æ€§:**
- æ”¯æŒ litellm çš„æ‰€æœ‰æä¾›å•†
- è‡ªåŠ¨åˆ†å—å¤„ç†å¤§å‹å†…å®¹
- æ™ºèƒ½å†…å®¹è¿‡æ»¤å’Œç»“æ„åŒ–è¾“å‡º
- å¯é…ç½®çš„æ¸©åº¦å’Œ token å‚æ•°

#### `save()` å‡½æ•° (`utils.py`)
ä½¿ç”¨é€‚å½“ç¼–ç å¤„ç†å°†å†…å®¹ä¿å­˜åˆ°æ–‡ä»¶ã€‚

**å‚æ•°:**
- `path`: ç›®å½•è·¯å¾„
- `name`: æ–‡ä»¶å
- `s`: å†…å®¹ (å­—ç¬¦ä¸²ã€å­—èŠ‚æˆ–å­—èŠ‚æ•°ç»„)
- `call`: é™„æœ‰ä¿å­˜æ–‡ä»¶è·¯å¾„çš„å›è°ƒå‡½æ•¸

#### `saveJson()` å‡½æ•° (`crawl.py`)
å¼‚æ­¥å‡½æ•¸ï¼Œç”¨äºä¿å­˜ä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯å¹¶å¤„ç†æ–‡ä»¶ä¸‹è½½ã€‚

**åŠŸèƒ½:**
- ä¿å­˜ `downloaded_files.json` å’Œæ–‡ä»¶å…ƒæ•°æ®
- å°†å¼•ç”¨çš„æ–‡ä»¶ä¸‹è½½å¹¶ä¿å­˜åˆ° `files/` å­ç›®å½•
- å¤±è´¥ä¸‹è½½çš„é”™è¯¯å¤„ç†

#### `crawl_config()` å‡½æ•° (`crawl.py`)
åŠ¨æ€é€‰æ‹©çˆ¬å–é…ç½®ï¼Œæ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦å¯ç”¨ LLM æ¨¡å¼ã€‚

**ç¯å¢ƒå˜é‡:**
- `CRAWL_MODE=llm`: å¯ç”¨ LLM å¢å¼ºæå–
- å…¶ä»–å€¼: ä½¿ç”¨ä¼ ç»Ÿ CSS é€‰æ‹©å™¨æå–

### æµè§ˆå™¨æœåŠ¡å‡½æ•°

#### `get_page_content()` (`browser_service.py`)
è·å–åŒ…å« HTMLã€æ–‡æœ¬ã€å…ƒæ•°æ®ã€é“¾æ¥å’Œå›¾ç‰‡çš„å®Œæ•´é¡µé¢å†…å®¹ã€‚

**å‚æ•°:**
- `url` (string, required): ç›®æ ‡é¡µé¢ URL
- `wait_for_selector` (string, optional): è·å–å†…å®¹å‰ç­‰å¾…ç‰¹å®šå…ƒç´ 
- `wait_timeout` (int, optional): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰(é»˜è®¤ 30000)
- `progress_callback` (callable, optional): å¯é€‰è¿›åº¦å›è°ƒ

#### `get_console_messages()` (`browser_service.py`)
æ•è·æ¥è‡ªç½‘é¡µçš„æ§åˆ¶å°æ¶ˆæ¯ (æ—¥å¿—ã€è­¦å‘Šã€é”™è¯¯)ã€‚

**å‚æ•°:**
- `url` (string, required): ç›®æ ‡é¡µé¢ URL
- `wait_for_selector` (string, optional): è·å–æ§åˆ¶å°æ¶ˆæ¯å‰ç­‰å¾…ç‰¹å®šå…ƒç´ 
- `wait_timeout` (int, optional): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰(é»˜è®¤ 30000)
- `progress_callback` (callable, optional): å¯é€‰è¿›åº¦å›è°ƒ

#### `get_network_requests()` (`browser_service.py`)
è®°å½•ç”±ç½‘é¡µå‘èµ·çš„æ‰€æœ‰ç½‘ç»œè¯·æ±‚å’Œå“åº”ã€‚

**å‚æ•°:**
- `url` (string, required): ç›®æ ‡é¡µé¢ URL
- `wait_for_selector` (string, optional): è·å–ç½‘ç»œè¯·æ±‚å‰ç­‰å¾…ç‰¹å®šå…ƒç´ 
- `wait_timeout` (int, optional): ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰(é»˜è®¤ 30000)
- `progress_callback` (callable, optional): å¯é€‰è¿›åº¦å›è°ƒ

## ğŸ¯ é…ç½®è¯´æ˜

### LLM æå–ç­–ç•¥

å¯ç”¨ LLM æ¨¡å¼æ—¶ï¼Œä½¿ç”¨ä»¥ä¸‹æ™ºèƒ½æå–é…ç½®ï¼š

**é»˜è®¤æå–æŒ‡ä»¤:**
```
You are a **Web Content Extraction Assistant**. Your task is to extract the **complete, clean, and precise main text content** from a given web page...
```

**LLM é…ç½®å‚æ•°:**
- `provider`: é€šè¿‡ `LLAMA_PROVIDER` ç¯å¢ƒå˜é‡é…ç½®
- `api_token`: é€šè¿‡ `LLAMA_API_TOKEN` ç¯å¢ƒå˜é‡é…ç½®
- `base_url`: é€šè¿‡ `LLAMA_BASE_URL` ç¯å¢ƒå˜é‡é…ç½®
- `max_tokens`: é»˜è®¤ 4096ï¼Œå¯é€šè¿‡ `LLAMA_MAX_TOKENS` è°ƒæ•´
- `temperature`: 0.1 (ç¡®ä¿è¾“å‡ºç¨³å®šæ€§)
- `chunk_token_threshold`: 1400 (åˆ†å—å¤„ç†é˜ˆå€¼)
- `apply_chunking`: true (å¯ç”¨å†…å®¹åˆ†å—)

### CSS æå–ç­–ç•¥

ä¼ ç»Ÿæ¨¡å¼ä½¿ç”¨é¢„é…ç½®çš„ CSS æå–æ¨¡å¼ï¼š

```javascript
{
  "baseSelector": "body",
  "fields": [
    {"name": "title", "selector": "h2", "type": "text"},
    {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
    {"name": "p", "selector": "p", "type": "text"}
  ]
}
```

### å†…å®¹è¿‡æ»¤

ä½¿ç”¨ `PruningContentFilter` é…ç½®ï¼š
- `threshold`: 0.35 (åŠ¨æ€é˜ˆå€¼)
- `min_word_threshold`: 3
- `threshold_type`: "dynamic"

### æµè§ˆå™¨é…ç½®

- æ— å¤´æ¨¡å¼å¯ç”¨
- JavaScript å¯ç”¨
- ç»•è¿‡ç¼“å­˜è·å–æ–°å†…å®¹
- Playwright é©±åŠ¨çš„è‡ªåŠ¨åŒ–
- å®æ—¶å†…å®¹ç›‘æ§

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

1. **é¢‘ç‡é™åˆ¶**: è¯·åˆç†æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆè¿‡åº¦å‹åŠ›
2. **robots.txt**: è¯·éµå®ˆç›®æ ‡ç½‘ç«™çš„ robots.txt è§„åˆ™
3. **æ³•å¾‹åˆè§„**: ç¡®ä¿çˆ¬å–è¡Œä¸ºç¬¦åˆç›¸å…³æ³•å¾‹ã€æ³•è§„å’Œç½‘ç«™æ¡æ¬¾
4. **æµè§ˆå™¨è¦æ±‚**: crawl4ai è¦æ±‚åœ¨ç³»ç»Ÿä¸Šå®‰è£…æµè§ˆå™¨å¼•æ“ (Chrome/Chromium)
5. **å†…å­˜ä½¿ç”¨**: å¤§å‹æˆªå›¾å’Œ PDF å¯èƒ½ä¼šæ¶ˆè€—å¤§é‡å†…å­˜å’Œç£ç›˜ç©ºé—´
6. **å®‰å…¨æ€§**: URL éªŒè¯é˜²æ­¢è®¿é—® localhost å’Œå†…ç½‘åœ°å€
7. **æµå¼å¤„ç†**: é•¿æ“ä½œæœŸé—´å®æ—¶æµå¼ä¼ è¾“è¿›åº¦æ›´æ–°

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æ­¤é¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ”— ç›¸å…³é“¾æ¥

- [crawl4ai å®˜æ–¹æ–‡æ¡£](https://github.com/unclecode/crawl4ai)
- [MCP åè®®è§„èŒƒ](https://modelcontextprotocol.io/)
- [Claude Desktop æ–‡æ¡£](https://docs.anthropic.com/claude/docs/overview)
- [LiteLLM æ–‡æ¡£](https://docs.litellm.ai/)
- [Ollama æ–‡æ¡£](https://github.com/ollama/ollama)
- [Playwright æ–‡æ¡£](https://playwright.dev/)
- [é¡¹ç›®åŒ…](https://pypi.org/project/dev-tool-mcp/)

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜æˆ–æœ‰å»ºè®®ï¼š

1. **æ£€æŸ¥ Issues**: [GitHub Issues](https://github.com/osins/dev-tool-mcp/issues)
2. **åˆ›å»ºæ–° Issue**: æŠ¥å‘Šé”™è¯¯æˆ–è¯·æ±‚åŠŸèƒ½
3. **å…ˆæµ‹è¯•**: è¿è¡Œ `python test/test_complete_crawler.py` ä»¥éªŒè¯è®¾ç½®

## ğŸ® CLI å…¥å£ç‚¹

åŒ…åŒ…å«ä¸€ä¸ª CLI å…¥å£ç‚¹ï¼š

```bash
# å®‰è£…å
dev-tool-mcp
```

è¿™ç›¸å½“äºè¿è¡Œï¼š
```bash
python -m mcp_server.server
```

## ğŸ› ï¸ å¯ç”¨å·¥å…·

---

**ä½¿ç”¨ crawl4aiã€Playwright å’Œ MCP åˆ¶ä½œ â¤ï¸**

*å½“å‰ç‰ˆæœ¬: 0.1.0*