# MCP Spider Server

An MCP (Model Context Protocol) spider server based on [crawl4ai](https://github.com/unclecode/crawl4ai) that provides powerful web crawling and content extraction capabilities.

## ğŸš€ Features

- **Intelligent Web Crawling**: Use crawl4ai for efficient web content extraction
- **Multi-format Output**: Support HTML, Markdown, JSON, PDF, and PNG formats
- **Screenshot Function**: Automatic webpage screenshot generation
- **PDF Export**: Export webpage content as PDF files
- **Content Filtering**: Use PruningContentFilter to optimize content extraction
- **Structured Data Extraction**: Support JsonCssExtractionStrategy for precise data extraction
- **File Downloads**: Automatically download and save referenced files
- **MCP Protocol Support**: Fully compatible with MCP standard, integrable with MCP-supporting clients

## ğŸ“¦ Installation

### Requirements

- Python 3.8+
- Virtual environment recommended

### Installation Steps

1. **Clone repository**
```bash
git clone https://github.com/osins/crawler-mcp-server.git
cd crawler-mcp-server
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows
```

3. **Install dependencies**
```bash
pip install -e .
```

## ğŸ”§ MCP Service Configuration

### Claude Desktop Configuration Example

Add the following configuration to Claude Desktop's configuration file:

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "spider": {
      "command": "/path/to/crawler-mcp-server/venv/bin/python",
      "args": [
        "/path/to/crawler-mcp-server/spider_mcp_server/server.py"
      ],
      "description": "MCP spider server using crawl4ai for web crawling and content extraction"
    }
  }
}
```

### General MCP Client Configuration

If you use other MCP clients, you can use the following general configuration:

```json
{
  "servers": {
    "spider-crawler": {
      "name": "Spider Crawler Server",
      "description": "Web crawling and content extraction server",
      "command": "/path/to/crawler-mcp-server/venv/bin/python",
      "args": [
        "/path/to/crawler-mcp-server/spider_mcp_server/server.py"
      ],
      "timeout": 30000
    }
  }
}
```

### ğŸ“‹ Configuration Notes

**Direct script execution is all you need:**
- No environment variables needed
- No need to use `-m` parameter
- Python automatically handles relative imports
- Most simple and reliable configuration

## ğŸ› ï¸ MCP Protocol Usage Guide

### Client Development Based on MCP Protocol

This project is based on the [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) protocol and provides standardized tool call interfaces. Here are the key points for writing MCP clients:

#### 1. MCP Connection Establishment

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Configure server parameters
server_params = StdioServerParameters(
    command="/path/to/venv/bin/python",  # Python interpreter path
    args=["/path/to/server.py"]         # Server script path
)

# Establish stdio connection
async with stdio_client(server_params) as (read, write):
    async with ClientSession(read, write) as session:
        await session.initialize()  # Initialize session
```

#### 2. Tool Calls and Result Handling

**âš ï¸ Important: MCP Return Value Structure**

The MCP server returns a `CallToolResult` object, with the actual content in `result.content`:

```python
# âŒ Incorrect approach (common error)
for content in result:  # result is not iterable
    print(content.text)

# âœ… Correct approach
result = await session.call_tool("tool_name", {"param": "value"})
for content in result.content:  # Access content attribute
    if content.type == "text":  # Check content type
        print(content.text)
```

#### 3. Tool Interfaces for This Project

**Available Tools:**
- `say_hello` - Test connection
- `echo_message` - Echo message  
- `crawl_web_page` - Web page crawling

**crawl_web_page Tool Parameters:**
```python
{
    "url": "https://example.com",           # URL to crawl
    "save_path": "./output_directory"       # Save path
}
```

**Return Value Handling:**
```python
result = await session.call_tool("crawl_web_page", {
    "url": "https://github.com/unclecode/crawl4ai",
    "save_path": "./results"
})

# Parse return results correctly
for content in result.content:
    if content.type == "text":
        message = content.text
        print(f"Crawling result: {message}")
        
        # Message format example:
        # "Successfully crawled https://github.com/unclecode/crawl4ai and saved 8 files to ./results/20231119-143022"
```

#### 4. Error Handling Best Practices

```python
async def safe_crawl(session: ClientSession, url: str, save_path: str):
    try:
        result = await session.call_tool("crawl_web_page", {
            "url": url,
            "save_path": save_path
        })
        
        # Check return results
        if result.content:
            for content in result.content:
                if content.type == "text":
                    if "Failed to crawl" in content.text:
                        print(f"âŒ Crawling failed: {content.text}")
                    else:
                        print(f"âœ… Crawling successful: {content.text}")
        else:
            print("âŒ No return result received")
            
    except Exception as e:
        print(f"âŒ Tool call failed: {e}")
```

## ğŸ› ï¸ Available Tools

### 1. `crawl_web_page`

Crawl webpage content from the specified URL and save in multiple formats.

**Parameters:**
- `url` (string, required): The webpage URL to crawl
- `save_path` (string, required): The directory path to save crawled content

**Features:**
- Automatic webpage screenshot (PNG)
- PDF export generation
- Raw Markdown content extraction
- Cleaned/filtered Markdown content
- Structured data extraction (JSON)
- HTML content preservation
- Downloaded files processing

**Example usage:**
```python
# Crawl webpage and save to specified directory
result = await session.call_tool("crawl_web_page", {
    "url": "https://example.com",
    "save_path": "./output_directory"
})
```

### 2. `say_hello`

Simple greeting tool for testing server connectivity.

**Parameters:**
- `name` (string, optional): Name to greet, defaults to "World"

**Example usage:**
```python
result = await session.call_tool("say_hello", {
    "name": "Alice"
})
```

### 3. `echo_message`

Echo messages back to test communication.

**Parameters:**
- `message` (string, required): The message to echo

**Example usage:**
```python
result = await session.call_tool("echo_message", {
    "message": "Hello MCP!"
})
```

## ğŸ“ Output File Structure

After crawling, the following files will be generated in the specified output directory:

```
output_directory/
â”œâ”€â”€ output.html              # Complete HTML content
â”œâ”€â”€ output.json              # Structured data (CSS-extracted JSON)
â”œâ”€â”€ output.png               # Webpage screenshot
â”œâ”€â”€ output.pdf               # PDF version of the page
â”œâ”€â”€ raw_markdown.md          # Raw markdown extraction
â”œâ”€â”€ fit_markdown.md          # Cleaned/filtered markdown
â”œâ”€â”€ downloaded_files.json    # List of downloaded files (if any)
â””â”€â”€ files/                   # Downloaded files directory (if any)
```

## ğŸ” Usage Examples

### Basic Web Crawling

```python
import asyncio
import os
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def crawl_example():
    # Configure server connection parameters (adjust paths as needed)
    project_root = Path("/path/to/your/crawler-mcp-server")
    server_params = StdioServerParameters(
        command=str(project_root / "venv" / "bin" / "python"),
        args=[str(project_root / "spider_mcp_server" / "server.py")]
    )
    
    # Create output directory
    output_dir = "./crawl_results"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # Connect to MCP server
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Initialize session
                await session.initialize()
                
                # Call crawling tool
                result = await session.call_tool("crawl_web_page", {
                    "url": "https://github.com/unclecode/crawl4ai",
                    "save_path": output_dir
                })
                
                # âœ… Correct return value handling
                # MCP server returns CallToolResult object, content is in result.content
                for content in result.content:
                    if content.type == "text":
                        print(f"âœ… Crawling result: {content.text}")
                        
    except Exception as e:
        print(f"âŒ Crawling failed: {e}")

# Run example
asyncio.run(crawl_example())
```

### Batch Crawling Multiple Webpages

```python
urls = [
    "https://example.com",
    "https://github.com", 
    "https://stackoverflow.com"
]

for i, url in enumerate(urls):
    result = await session.call_tool("crawl_web_page", {
        "url": url,
        "save_path": f"./results/crawl_{i+1}"
    })
    
    # âœ… Correct return value handling
    for content in result.content:
        if content.type == "text":
            print(f"Crawled: {url} - {content.text}")
    
    # Add delay to avoid too frequent requests
    await asyncio.sleep(2)
```

## ğŸ§ª Development and Testing

### Run Tests

The project includes a comprehensive test suite:

```bash
# Run complete crawler test with real file output
python test/test_complete_crawler.py

# Run individual component tests
python test/test_hello.py      # Test hello/echo functions
python test/test_server.py     # Test MCP server functionality
python test/test_crawl.py      # Test crawling functions
python test/test_complete.py    # Test complete workflow
```

### Test Directory Structure

```
test/
â”œâ”€â”€ test_complete_crawler.py    # Full integration test
â”œâ”€â”€ test_hello.py             # Hello/echo functionality
â”œâ”€â”€ test_server.py            # MCP server protocol
â”œâ”€â”€ test_crawl.py             # Core crawling logic
â””â”€â”€ test_complete.py          # End-to-end workflow
```

### Development Mode

```bash
# Install development dependencies
pip install -e ".[dev]"

# The project uses pyright for type checking (configured in pyproject.toml)
# No additional linting tools are configured
```

## ğŸ“š Project Structure

```
crawler-mcp-server/
â”œâ”€â”€ spider_mcp_server/          # Main package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ server.py              # MCP server implementation
â”‚   â”œâ”€â”€ crawl.py              # Crawling logic and file handling
â”‚   â””â”€â”€ utils.py              # Utility functions for file I/O
â”œâ”€â”€ test/                     # Test suite
â”œâ”€â”€ test_output/              # Test output directory
â”œâ”€â”€ typings/                  # Type stubs for crawl4ai
â”œâ”€â”€ pyproject.toml            # Project configuration
â””â”€â”€ README.md                # This file
```

## ğŸ“š API Reference

### Core Classes and Functions

#### `save()` function (`utils.py`)
Save content to files with proper encoding handling.

**Parameters:**
- `path`: Directory path
- `name`: Filename
- `s`: Content (string, bytes, or bytearray)
- `call`: Callback function with saved file path

#### `saveJson()` function (`crawl.py`)
Async function to save downloaded files information and handle file downloads.

**Features:**
- Saves `downloaded_files.json` with file metadata
- Downloads and saves referenced files to `files/` subdirectory
- Error handling for failed downloads

## ğŸ¯ Configuration Details

### CSS Extraction Strategy

The crawler uses a pre-configured CSS extraction schema:

```javascript
{
  "baseSelector": "body",
  "fields": [
    {"name": "title", "selector": "h2", "type": "text"},
    {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
    {"name": "p", "selector": "p", "type": "text"}
  ]
}
```

### Content Filtering

Uses `PruningContentFilter` with these settings:
- `threshold`: 0.35 (dynamic threshold)
- `min_word_threshold`: 3
- `threshold_type`: "dynamic"

### Browser Configuration

- Headless mode enabled
- JavaScript enabled
- Bypass cache for fresh content

## âš ï¸ Important Notes

1. **Rate Limiting**: Please control crawling frequency reasonably to avoid excessive pressure on target websites
2. **robots.txt**: Please comply with robots.txt rules of target websites
3. **Legal Compliance**: Ensure crawling behavior complies with relevant laws, regulations and website terms of use
4. **Browser Requirements**: crawl4ai requires a browser engine (Chrome/Chromium) to be installed on the system
5. **Memory Usage**: Large screenshots and PDFs may consume significant memory and disk space

## ğŸ¤ Contributing

Welcome to submit Issues and Pull Requests!

1. Fork this repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“„ License

This project uses MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ”— Related Links

- [crawl4ai Official Documentation](https://github.com/unclecode/crawl4ai)
- [MCP Protocol Specification](https://modelcontextprotocol.io/)
- [Claude Desktop Documentation](https://docs.anthropic.com/claude/docs/overview)
- [Project Package](https://pypi.org/project/spider-mcp/)

## ğŸ“ Support

If you encounter problems or have suggestions:

1. **Check Issues**: [GitHub Issues](https://github.com/osins/crawler-mcp-server/issues)
2. **Create New Issue**: Report bugs or request features
3. **Test First**: Run `python test/test_complete_crawler.py` to verify setup

## ğŸ® CLI Entry Point

The package includes a CLI entry point:

```bash
# After installation
spider-mcp
```

This is equivalent to running:
```bash
python -m spider_mcp_server.server
```

## ğŸ› ï¸ Available Tools

---

**Made with â¤ï¸ using crawl4ai and MCP**

*Current Version: 0.1.0*