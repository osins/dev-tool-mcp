# MCP Spider Server

ä¸€ä¸ªåŸºäº [crawl4ai](https://github.com/unclecode/crawl4ai) çš„ MCP (Model Context Protocol) çˆ¬è™«æœåŠ¡å™¨ï¼Œæä¾›å¼ºå¤§çš„ç½‘é¡µæŠ“å–å’Œå†…å®¹æå–åŠŸèƒ½ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- **æ™ºèƒ½ç½‘é¡µæŠ“å–**ï¼šä½¿ç”¨ crawl4ai è¿›è¡Œé«˜æ•ˆçš„ç½‘é¡µå†…å®¹æå–
- **å¤šæ ¼å¼è¾“å‡º**ï¼šæ”¯æŒ Markdownã€HTMLã€JSON ç­‰å¤šç§æ ¼å¼
- **æˆªå›¾åŠŸèƒ½**ï¼šè‡ªåŠ¨ç”Ÿæˆç½‘é¡µæˆªå›¾
- **PDF å¯¼å‡º**ï¼šå°†ç½‘é¡µå†…å®¹å¯¼å‡ºä¸º PDF æ–‡ä»¶
- **å†…å®¹è¿‡æ»¤**ï¼šä½¿ç”¨ PruningContentFilter ä¼˜åŒ–å†…å®¹æå–
- **ç»“æ„åŒ–æ•°æ®æå–**ï¼šæ”¯æŒ JsonCssExtractionStrategy è¿›è¡Œç²¾ç¡®æ•°æ®æå–
- **MCP åè®®æ”¯æŒ**ï¼šå®Œå…¨å…¼å®¹ MCP æ ‡å‡†ï¼Œå¯ä¸æ”¯æŒ MCP çš„å®¢æˆ·ç«¯é›†æˆ

## ğŸ“¦ å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- æ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ

### å®‰è£…æ­¥éª¤

1. **å…‹éš†ä»“åº“**
```bash
git clone https://github.com/osins/crawler-mcp-server.git
cd crawler-mcp-server
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows
```

3. **å®‰è£…ä¾èµ–**
```bash
pip install -e .
```

## ğŸ”§ MCP æœåŠ¡é…ç½®

### Claude Desktop é…ç½®ç¤ºä¾‹

å°†ä»¥ä¸‹é…ç½®æ·»åŠ åˆ° Claude Desktop çš„é…ç½®æ–‡ä»¶ä¸­ï¼š

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "spider": {
      "command": "python",
      "args": [
        "/Users/shaoyingwang/works/codes/mcp/spider/spider_mcp_server/server.py"
      ],
      "description": "MCP spider server using crawl4ai for web crawling and content extraction",
      "env": {
        "PYTHONPATH": "/Users/shaoyingwang/works/codes/mcp/spider"
      }
    }
  }
}
```

### MCP å®¢æˆ·ç«¯é…ç½®ç¤ºä¾‹

å¦‚æœæ‚¨ä½¿ç”¨å…¶ä»– MCP å®¢æˆ·ç«¯ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹é€šç”¨é…ç½®ï¼š

```json
{
  "servers": {
    "spider-crawler": {
      "name": "Spider Crawler Server",
      "description": "Web crawling and content extraction server",
      "command": "python",
      "args": [
        "/path/to/crawler-mcp-server/spider_mcp_server/server.py"
      ],
      "environment": {
        "PYTHONPATH": "/path/to/crawler-mcp-server",
        "CRAWL4AI_LOG_LEVEL": "INFO"
      },
      "timeout": 30000
    }
  }
}
```

### ç¯å¢ƒå˜é‡é…ç½®

å¯é€‰çš„ç¯å¢ƒå˜é‡ï¼š

```bash
# è®¾ç½® crawl4ai æ—¥å¿—çº§åˆ«
export CRAWL4AI_LOG_LEVEL=INFO

# è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º test_outputï¼‰
export SPIDER_OUTPUT_DIR=/path/to/output

# è®¾ç½®ç”¨æˆ·ä»£ç†
export CRAWL4AI_USER_AGENT="MCP Spider Bot 1.0"
```

## ğŸ› ï¸ å¯ç”¨å·¥å…·

### 1. `crawl_web_page`

æŠ“å–æŒ‡å®š URL çš„ç½‘é¡µå†…å®¹ã€‚

**å‚æ•°ï¼š**
- `url` (string, å¿…éœ€): è¦æŠ“å–çš„ç½‘é¡µ URL
- `output_dir` (string, å¯é€‰): è¾“å‡ºç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º "test_output"

**åŠŸèƒ½ï¼š**
- è‡ªåŠ¨ç”Ÿæˆç½‘é¡µæˆªå›¾ (PNG)
- å¯¼å‡º PDF ç‰ˆæœ¬
- ç”Ÿæˆ Markdown æ ¼å¼å†…å®¹
- æå–ç»“æ„åŒ–æ•°æ® (JSON)

**ç¤ºä¾‹ä½¿ç”¨ï¼š**
```python
# æŠ“å–ç½‘é¡µå¹¶ä¿å­˜åˆ°é»˜è®¤ç›®å½•
result = await session.call_tool("crawl_web_page", {
    "url": "https://example.com"
})

# æŒ‡å®šè¾“å‡ºç›®å½•
result = await session.call_tool("crawl_web_page", {
    "url": "https://example.com",
    "output_dir": "/path/to/custom/output"
})
```

### 2. `say_hello`

ç®€å•çš„é—®å€™å·¥å…·ï¼Œç”¨äºæµ‹è¯•è¿æ¥ã€‚

**å‚æ•°ï¼š** æ— 

**ç¤ºä¾‹ä½¿ç”¨ï¼š**
```python
result = await session.call_tool("say_hello", {})
```

### 3. `echo_message`

å›æ˜¾æ¶ˆæ¯ï¼Œç”¨äºæµ‹è¯•é€šä¿¡ã€‚

**å‚æ•°ï¼š**
- `message` (string, å¿…éœ€): è¦å›æ˜¾çš„æ¶ˆæ¯

**ç¤ºä¾‹ä½¿ç”¨ï¼š**
```python
result = await session.call_tool("echo_message", {
    "message": "Hello MCP!"
})
```

## ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„

æŠ“å–å®Œæˆåï¼Œä¼šåœ¨æŒ‡å®šçš„è¾“å‡ºç›®å½•ä¸­ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
output_directory/
â”œâ”€â”€ example_com.png          # ç½‘é¡µæˆªå›¾
â”œâ”€â”€ example_com.pdf          # PDF ç‰ˆæœ¬
â”œâ”€â”€ example_com.md          # Markdown å†…å®¹
â”œâ”€â”€ example_com_cleaned.md  # æ¸…ç†åçš„ Markdown
â”œâ”€â”€ example_com.json        # ç»“æ„åŒ–æ•°æ®
â”œâ”€â”€ example_com_iframes.md  # iframe å†…å®¹
â”œâ”€â”€ example_com_links.md    # é“¾æ¥åˆ—è¡¨
â””â”€â”€ example_com_images.md   # å›¾ç‰‡åˆ—è¡¨
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ç½‘é¡µæŠ“å–

```python
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def crawl_example():
    # è¿æ¥åˆ° MCP æœåŠ¡å™¨
    server_params = StdioServerParameters(
        command="python",
        args=["/path/to/spider_mcp_server/server.py"]
    )
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # åˆå§‹åŒ–ä¼šè¯
            await session.initialize()
            
            # æŠ“å–ç½‘é¡µ
            result = await session.call_tool("crawl_web_page", {
                "url": "https://github.com/unclecode/crawl4ai",
                "output_dir": "./crawl_results"
            })
            
            print("æŠ“å–å®Œæˆï¼")
            print(f"ç»“æœ: {result.content[0].text}")

# è¿è¡Œç¤ºä¾‹
asyncio.run(crawl_example())
```

### æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ

```python
urls = [
    "https://example.com",
    "https://github.com",
    "https://stackoverflow.com"
]

for url in urls:
    result = await session.call_tool("crawl_web_page", {
        "url": url,
        "output_dir": f"./results/{url.replace('https://', '').replace('/', '_')}"
    })
    print(f"å·²æŠ“å–: {url}")
```

## ğŸ§ª å¼€å‘å’Œæµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•
python test_complete_crawler.py

# æˆ–è€…ä½¿ç”¨ pytestï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
pytest test/
```

### å¼€å‘æ¨¡å¼

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œä»£ç æ£€æŸ¥
flake8 spider_mcp_server/
black spider_mcp_server/
```

## ğŸ“š API å‚è€ƒ

### æ ¸å¿ƒç±»å’Œå‡½æ•°

#### `Crawl4aiExtractor`
ä¸»è¦çš„çˆ¬è™«æå–å™¨ç±»ï¼Œå°è£…äº† crawl4ai çš„åŠŸèƒ½ã€‚

**ä¸»è¦æ–¹æ³•ï¼š**
- `extract_content(url: str) -> dict`: æå–ç½‘é¡µå†…å®¹
- `save_results(result: dict, output_dir: str, url: str)`: ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

#### `save_markdown_file(content: str, file_path: str) -> str`
ä¿å­˜ Markdown å†…å®¹åˆ°æ–‡ä»¶

#### `save_binary_file(data: bytes, file_path: str) -> str`
ä¿å­˜äºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚æˆªå›¾ã€PDFï¼‰åˆ°æ–‡ä»¶

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **é¢‘ç‡é™åˆ¶**ï¼šè¯·åˆç†æ§åˆ¶æŠ“å–é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆè¿‡å¤§å‹åŠ›
2. **robots.txt**ï¼šè¯·éµå®ˆç›®æ ‡ç½‘ç«™çš„ robots.txt è§„åˆ™
3. **æ³•å¾‹åˆè§„**ï¼šç¡®ä¿æŠ“å–è¡Œä¸ºç¬¦åˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾
4. **ä¾èµ–ç¯å¢ƒ**ï¼šç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ç³»ç»Ÿä¾èµ–ï¼ˆå¦‚ Chrome/Chromiumï¼‰

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ”— ç›¸å…³é“¾æ¥

- [crawl4ai å®˜æ–¹æ–‡æ¡£](https://github.com/unclecode/crawl4ai)
- [MCP åè®®è§„èŒƒ](https://modelcontextprotocol.io/)
- [Claude Desktop æ–‡æ¡£](https://docs.anthropic.com/claude/docs/overview)

## ğŸ“ æ”¯æŒ

å¦‚æœæ‚¨é‡åˆ°é—®é¢˜æˆ–æœ‰å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ [Issues](https://github.com/osins/crawler-mcp-server/issues) é¡µé¢
2. åˆ›å»ºæ–°çš„ Issue æè¿°æ‚¨çš„é—®é¢˜
3. è”ç³»ç»´æŠ¤è€…

---

**Made with â¤ï¸ using crawl4ai and MCP**